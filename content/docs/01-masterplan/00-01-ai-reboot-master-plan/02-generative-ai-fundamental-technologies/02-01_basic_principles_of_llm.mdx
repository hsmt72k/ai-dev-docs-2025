---
title: LLM の基本原理
description: Basic Principles of LLM
icon: Scale
---

## 大規模言語モデル（LLM）の基本原理

## 🔑 エグゼクティブサマリー

大規模言語モデル（LLM）は、トークン化、コンテキストウィンドウ、確率的推論の3つの基本原理に基づいて動作します。
トークン化はテキストを小さな単位に分割し、コンテキストウィンドウは処理できる情報量を定義し、推論メカニズムは次のトークンを予測します。
これらの要素が連携することで、人間のような自然な言語理解と生成が実現されています。

## 本ドキュメントについて

### 想定読者
- LLM の基本的な仕組みを理解したい技術者・非技術者
- AI 関連プロジェクトに関わる初級〜中級レベルの開発者
- 大規模言語モデルの仕組みに興味がある一般ユーザー

### 対象範囲
- 一般的な LLM（GPT、Claude、Llama など）の基本原理
- 実装の詳細ではなく、概念的な理解に焦点

## LLM の基本原理

大規模言語モデル（LLM: Large Language Model）は現代の AI 技術の中核をなす存在です。
その仕組みは複雑ですが、主に以下の3つの基本原理によって支えられています。

LLM の全体像は複雑に見えますが、実は以下の3つの原理「①テキストの分割（トークン化）、②文脈の保持（コンテキスト）、③言語の生成（推論）」に還元できます。
本ドキュメントでは、この3原理を軸に LLM の仕組みをシンプルに理解していきます。

### トークン化とは（Tokenization）

**トークン化とは**: テキストを LLM が処理できる最小単位（トークン）に分割するプロセスです。これは人間の言葉を機械が理解できる形に変換する第一歩です。

LLM の動作は、まずテキストを「機械が処理しやすい単位」に変換するところから始まります。これがトークン化プロセスです。

<Callout type="info" title="初心者向け解説">
トークン化は料理のレシピを考えると理解しやすいかもしれません。

長い料理の手順書（テキスト）を、「材料を切る」「炒める」「煮る」といった個別の調理ステップ（トークン）に分解するようなものです。
これにより、コンピューターが理解・処理しやすい単位に分けられます。
</Callout>

#### トークン化の仕組み

トークン化の主な特徴は以下の通りです。

1. **分割単位**: テキストは単語、部分語、文字、または記号などの単位に分割されます
2. **言語依存性**: 英語では単語や部分語がトークンになりやすいですが、日本語などでは文字や短い文節がトークンになることが多いです
3. **頻度ベース**: 一般的な LLM では、出現頻度に基づいて効率的なトークン分割を行います

#### トークン化の例と実際の応用

英語の例: `"I love artificial intelligence" → ["I", " love", " artificial", " intel", "ligence"]`
※この例はGPT系のBPEトークナイザーによる分割例です。モデルによって分割方法は異なります。

日本語の例: `"人工知能が好きです" → ["人工", "知能", "が", "好き", "です"]`
※日本語は言語構造上、英語より多くのトークンに分割される傾向があります。

**ビジネス応用例**: 多言語対応チャットボットの開発において、日本語と英語のトークン化効率の違いを考慮することで、処理コストを最適化。例えば、日本語の応答は英語の約1.5倍のトークン数が必要なため、料金プランやリソース配分を調整することで、コスト効率の良いサービス提供が可能になります。

トークン化の概念を理解したところで、次にLLMがどのようにこれらのトークンを「記憶」するかを見ていきましょう。コンテキストウィンドウは、そのための重要なメカニズムです。

**図1：トークン化処理の構造**

※ 色の意味：青＝入力データ、緑＝処理プロセス、黄＝中間データ、赤＝参照データ、紫＝次工程

この図はテキストがトークン化される全体的なプロセスを表しています。
ユーザーの入力テキストがトークン辞書を参照しながら処理され、モデルが理解できるトークンのシーケンスに変換される流れを可視化しています。

import { Mermaid } from "@/components/mdx/mermaid";

<Mermaid chart={`
graph TD
    A[テキスト入力] --> B[トークン化プロセス]
    B --> C[トークンシーケンス]
    D[トークン辞書] --> B
    C --> E[モデル処理へ]
    style A fill:#D4F1F9,stroke:#1A5276,color:#000
    style B fill:#D5F5E3,stroke:#1E8449,color:#000
    style C fill:#FCF3CF,stroke:#B7950B,color:#000
    style D fill:#FADBD8,stroke:#943126,color:#000
    style E fill:#E8DAEF,stroke:#6C3483,color:#000
`} />

### コンテキストウィンドウとは（Context Window）

**コンテキストウィンドウとは**: LLM が一度に考慮できるテキストの量を定義する「記憶」の容量制限です。会話や文書内で参照できる情報の範囲を決定します。

情報の「短期記憶」とも言えるコンテキストウィンドウは、LLM が一度に処理できる情報量を決定し、モデルの能力を大きく左右します。

<Callout type="info" title="初心者向け解説">
コンテキストウィンドウは、人間の「作業記憶」に似ています。

例えば、あなたが長い小説を読むとき、現在読んでいるページの内容と、ある程度前のページの内容は覚えていますが、100 ページ前の細かい描写はすぐには思い出せないでしょう。
LLM も同様に、一定量のトークン（情報）しか同時に「記憶」できません。この「記憶容量」がコンテキストウィンドウです。
</Callout>

#### コンテキストウィンドウの特徴

1. **サイズの制限**: モデルによって異なりますが、数千～数百万トークンの範囲です
2. **情報の保持**: ユーザー入力と過去の対話履歴を含みます
3. **文脈理解**: 長いコンテキストウィンドウにより、より広範な文脈の理解が可能になります

#### コンテキストウィンドウの具体例

以下は、コンテキストウィンドウがどのように機能するかを示す簡単な例です。

**例：Claude（約100Kトークン）と GPT-3.5（約4Kトークン）のコンテキストウィンドウの違い**

例えば、300ページの法律文書を分析する場合：
- Claude は文書全体の約30%（90ページ程度）を一度に「記憶」できます
- GPT-3.5 は文書のわずか約4%（12ページ程度）しか一度に「記憶」できません

**会話の例**：
```md
[ユーザー]: 太陽系の惑星について教えてください。

[AI]     : 太陽系には水星、金星、地球、火星、木星、土星、天王星、海王星の8つの惑星があります。
           冥王星は2006年に準惑星に分類変更されました...（詳細な説明が続く）

[ユーザー]: 地球について詳しく教えてください。

[AI]     : 地球は太陽から3番目の惑星で、直径は約12,742km、表面の約71%が水で覆われています...
           （詳細な説明が続く）

[ユーザー]: 大きさ順に並べるとどうなりますか？

[AI]     : 太陽系の惑星を大きさ（直径）順に並べると以下のようになります：
             1. 木星（約139,820km）
             2. 土星（約116,460km）
             ...
```

このような会話では、ユーザーが「大きさ順に並べると」と質問したとき、コンテキストウィンドウに十分な容量があれば、
AI は最初の質問と回答（太陽系の惑星の情報）を覚えていて、それに基づいて回答できます。

しかし、コンテキストウィンドウが小さいモデルや、非常に長い会話の後では、初期の情報が「忘れられて」しまい、正確に答えられなくなる可能性があります。

#### 主要 LLM のコンテキストウィンドウサイズ

| モデル名 | コンテキストウィンドウサイズ | 備考 |
|---------|----------------------|------|
| **Gemini 1.5 Pro** | 約 1,000,000 トークン | 現時点で最大級 |
| **Claude 3 Opus** | 約 200,000 トークン | Anthropic 社製 |
| **GPT-4 Turbo** | 約 128,000 トークン | OpenAI 社製 |
| **Llama 3.1** | 約 128,000 トークン | Meta 社のオープンソースモデル |
| **Llama 3** | 約 8,192 トークン | Meta 社製 |
| **Llama 2** | 約 4,096 トークン | Meta 社製、初期バージョン |
| **Grok-1.5** | 約 128,000 トークン | xAI 社製 |
| **Grok-1** | 約 8,192 トークン | xAI 社製 |
| **Gemini 1.0 Ultra** | 約 32,000 トークン | Google 社製 |
| **Mistral Large** | 約 32,000 トークン | Mistral AI 社製 |
| **DeepSeek Coder** | 約 16,000 トークン | コード特化モデル |
| **Perplexity** | 約 12,000〜100,000 トークン | モデルによって異なる |
| **GPT-3.5** | 約 4,000 トークン | OpenAI 社製、広く利用されている |

#### 実用的なユースケース

**企業の知識ベース構築**: 製造業の企業が、10年分の技術マニュアル（合計10,000ページ）を LLM に取り込み、社内の技術サポートシステムを構築。
大きなコンテキストウィンドウを持つモデル（Claude 3 Opus）を採用することで、質問に対して適切なマニュアルの該当箇所を即座に参照し、正確な回答を提供。
これにより新入社員のトレーニング時間が30%短縮され、技術サポートの応答時間が60%改善しました。

コンテキストウィンドウでトークンを保持できるようになった LLM は、次に何をするのでしょうか？
ここで、次に何を言うべきかを決定する「推論」のメカニズムが登場します。

**図2：コンテキストウィンドウの構造と情報フロー**

※ 色の意味：青＝過去データ、緑＝現在処理、黄＝入力データ、紫＝処理プロセス、赤＝出力データ、オレンジ＝廃棄データ

この図はコンテキストウィンドウの動作原理を示しています。
過去の対話履歴と現在の入力がコンテキストウィンドウに取り込まれ、LLMの推論処理に使用される一方、ウィンドウサイズを超えた情報は「忘却」される仕組みを表現しています。

<Mermaid chart={`
graph LR
    A[過去の対話] --> B[コンテキスト<br/>ウィンドウ]
    C[現在の入力] --> B
    B --> D[LLMの推論処理]
    D --> E[応答生成]
    F[ウィンドウ外の情報] -.-> |忘れられる| B
    style A fill:#D4F1F9,stroke:#1A5276,color:#000
    style B fill:#D5F5E3,stroke:#1E8449,color:#000
    style C fill:#FCF3CF,stroke:#B7950B,color:#000
    style D fill:#E8DAEF,stroke:#6C3483,color:#000
    style E fill:#FADBD8,stroke:#943126,color:#000
    style F fill:#F5CBA7,stroke:#A04000,color:#000
`} />

### 推論とは（Inference）

**推論とは**: LLM が与えられた入力に基づいて、次に続く可能性が最も高いテキストを予測し生成するプロセスです。これは「次の単語は何か？」を確率的に判断する仕組みです。

LLM の「思考」とも言える推論プロセスは、確率計算に基づいて次のトークンを予測し、自然な文章を生成する鍵となる機能です。

<Callout type="info" title="初心者向け解説">
推論は、クロスワードパズルを解くようなものだと考えてみましょう。

たとえば「日本の首都は東＿」という文があれば、「京」が入る確率が非常に高いことが分かるかと思います。
LLM はこれを大規模なデータから学習し、どの単語（トークン）が次に来る可能性が高いかを計算します。

単純な穴埋めではなく、文脈全体から判断するので、より複雑な予測も可能になります。
</Callout>

#### 推論プロセスの流れ

1. **入力処理**: ユーザーの入力がトークン化され、コンテキストウィンドウに追加されます
2. **確率計算**: モデルは各トークンの後に続く可能性のあるすべてのトークンの確率を計算します
3. **トークン選択**: 確率分布に基づいて次のトークンが選択されます
4. **反復処理**: 選択されたトークンがコンテキストに追加され、プロセスが繰り返されます
5. **停止条件**: 特定の停止条件（最大長、特定のトークンなど）に達するまで続きます

#### 推論の制御パラメータ

推論プロセスは様々なパラメータによって制御されます。

1. **温度（Temperature）**: 応答のランダム性を制御します。低温では決定論的に、高温では創造的になります。
   - 温度 0.2 の例: 「おはようございます」→「おはようございます。今日はどのようなご用件でしょうか？」（予測可能で安定した応答）
   - 温度 0.8 の例: 「おはようございます」→「おはようございます！太陽が輝く素晴らしい朝ですね。今日はどんな冒険が待っているでしょうか？」（より創造的で多様な応答）

2. **Top-p サンプリング（核サンプリング）**: 確率の高いトークンのみを考慮し、低確率のトークンを除外します。
   - 動作原理: 確率順にトークンを並べ、累積確率が設定した閾値（p）に達するまでのトークンのみを候補とします
   - Top-p 0.5 の例: 「日本の首都は」→「東京」（高確率の「東京」のみが候補に）
   - Top-p 0.9 の例: 「私の趣味は」→「読書」「旅行」「音楽」「スポーツ」など多様な選択肢から選ばれる（確率上位の複数候補が対象に）

#### 実用的なユースケース

**金融レポート生成の最適化**: 投資銀行が四半期ごとの市場分析レポートの生成を LLM で自動化。クライアント向けの正確な財務データ分析には低温度設定（0.2）を使用し、市場動向の予測や投資戦略の提案には中程度の温度設定（0.6）を適用することで、データの正確性を保ちながらも洞察に富んだ提案を含むバランスの取れたレポートを作成。これにより分析レポート作成時間が従来の2日間から2時間に短縮され、アナリストがより高度な分析に集中できるようになりました。

さて、ここまでトークン化、コンテキストウィンドウ、推論という3つの基本原理をそれぞれ見てきました。しかし、これらは独立して機能するわけではありません。次のセクションでは、これら3つの原理がどのように連携して、全体としてのLLMの言語理解・生成能力を支えているかを見ていきましょう。

**図3：推論の流れとパラメータ**

※ 色の意味：青＝入力データ、緑＝処理プロセス、黄＝中間データ、赤＝制御パラメータ、紫＝出力データ、オレンジ＝繰り返し処理

この図は推論プロセスの全体像を示しています。
コンテキストウィンドウから入力された情報がトランスフォーマーアーキテクチャで処理され、次のトークンの確率分布が計算されます。
その後、温度や Top-p といったパラメータによって制御されながらトークンが選択され、最終的な出力が生成される流れを可視化しています。

<Mermaid chart={`
graph TD
    A[コンテキスト<br/>ウィンドウ] --> B[トランスフォーマー<br/>アーキテクチャ]
    B --> C[次トークンの<br/>確率分布]
    C --> D[サンプリング<br/>方法]
    X1[温度（Temperature）] --> D
    X2[Top-p] --> D
    D --> E[トークン選択]
    E --> F[出力に追加]
    F -- 続行 --> G[コンテキスト更新]
    G --> B
    F -- 完了 --> H[最終出力]
    style A fill:#D4F1F9,stroke:#1A5276,color:#000
    style B fill:#D5F5E3,stroke:#1E8449,color:#000
    style C fill:#FCF3CF,stroke:#B7950B,color:#000
    style D fill:#FADBD8,stroke:#943126,color:#000
    style E fill:#E8DAEF,stroke:#6C3483,color:#000
    style F fill:#F5CBA7,stroke:#A04000,color:#000
    style G fill:#D6EAF8,stroke:#2874A6,color:#000
    style H fill:#D7BDE2,stroke:#6C3483,color:#000
    style X1 fill:#FADBD8,stroke:#943126,color:#000
    style X2 fill:#FADBD8,stroke:#943126,color:#000
`} />

## 3つの原理の連携

これら3つの原理は独立して機能するのではなく、相互に連携することで LLM の自然な言語理解と生成能力を支えています。

### 相互依存性とその影響

3つの原理の相互依存関係は LLM の性能に直接影響します：

- **トークン化とコンテキストウィンドウの関係**: 効率的なトークン化によって限られたコンテキストウィンドウ内により多くの情報を詰め込むことができます。例えば、日本語と英語の混合テキストでは、トークン化効率の違いにより情報密度に差が生じ、実質的なコンテキスト容量が変動します。

- **コンテキストウィンドウと推論精度の関係**: コンテキストウィンドウが小さすぎると、重要な文脈情報が失われ、推論の質が低下します。たとえば、法律文書の分析で前の章に定義された専門用語がコンテキストから外れると、正確な解釈ができなくなることがあります。

- **トークン化と推論の関係**: 言語特有のトークン化特性が推論の質に影響します。例えば、専門用語が適切にトークン化されないと、その領域での推論精度が低下します。医療や法律など専門分野では、特殊なトークン辞書の使用が重要になることがあります。

このような相互依存性を理解することで、LLM の能力を最大限に活用するためのプロンプト設計や運用戦略が立てられます。

**図4：3つの基本原理の連携プロセス全体図**

※ 色の意味：青＝入力、緑＝トークン化処理、黄＝中間データ、赤＝コンテキスト処理、紫＝推論処理、オレンジ＝出力生成

この図は3つの基本原理が実際にどのように連携して動作するかを一連のフローとして表現しています。
テキスト入力からトークン化、コンテキストへの追加、推論処理を経て、最終的に文の生成に至るまでの全体の流れが一目でわかるよう設計されています。

<Mermaid chart={`
graph TD
    A[テキスト入力] --> B[トークン化]
    B --> C[トークン列]
    C --> D[コンテキストに追加]
    D --> E[推論処理（確率計算）]
    E --> F[出力トークン]
    F --> G[文生成]
    style A fill:#D4F1F9,stroke:#1A5276,color:#000
    style B fill:#D5F5E3,stroke:#1E8449,color:#000
    style C fill:#FCF3CF,stroke:#B7950B,color:#000
    style D fill:#FADBD8,stroke:#943126,color:#000
    style E fill:#E8DAEF,stroke:#6C3483,color:#000
    style F fill:#F5CBA7,stroke:#A04000,color:#000
    style G fill:#D7BDE2,stroke:#6C3483,color:#000
`} />

### 連携の実用例: 医療診断支援システム

大学病院で導入された医療診断支援システムでは、3つの原理が以下のように連携しています:

1. **トークン化**: 患者の症状記述や検査結果を医療用語とその関連語に正確にトークン化。医学用語のトークン化効率を高めるために特殊なトークナイザーを使用し、専門用語の認識精度を向上。

2. **コンテキストウィンドウ**: 患者の過去10年分の医療記録、現在の症状、検査結果、最新の医学文献を同時に参照できる128Kトークンのコンテキストウィンドウを活用。これにより、慢性疾患の長期的パターンや複雑な症例の全体像を把握。

3. **推論メカニズム**: 診断提案では低温度設定（0.1）を使用して確実性の高い一般的な診断を提示し、稀少疾患の可能性検討では高い温度設定（0.7）を使用して幅広い可能性を考慮。

このシステム導入により、稀少疾患の初期診断の正確性が35%向上し、診断に要する時間が平均40%短縮されました。
特に複雑な症例において、医師の診断精度を支援する重要なツールとなっています。

ここまで3つの基本原理とその連携について見てきました。次に、これらの原理を理解した上で認識しておくべきLLMの実際の限界と注意点について検討しましょう。

### 技術的制約と課題

LLM には以下のような技術的制約と課題があり、実際の応用において考慮する必要があります。

- **コンテキストウィンドウの限界**:
  - 問題例: 300ページの法律文書を分析する場合、4,000トークンのコンテキストウィンドウを持つモデルでは全体の約5%しか一度に処理できません
  - 対処法: 文書を要約・分割して処理し、結果を統合する手法や、コンテキストウィンドウの大きい最新モデルを使用する

- **計算コスト**:
  - 問題例: 1,000,000トークンのコンテキストウィンドウを使用すると、4,000トークンの場合と比較して約250倍の計算リソースと応答時間が必要になる場合があります
  - 対処法: 必要な文脈のみを抽出するプロンプト設計や、費用対効果を考慮したモデル選択（小さいタスクには小さいモデル）

- **幻覚（Hallucination）**:
  - 問題例: 「2023年の世界経済フォーラムの議長は誰ですか？」という質問に対して、学習データの期間外の情報を自信を持って（しかし不正確に）生成
  - 対処法: 回答の根拠を明示するよう指示する、学習データの期間外の質問には回答できないことを明示するプロンプト設計

## まとめ

LLM の基本原理であるトークン化、コンテキストウィンドウ、推論メカニズムは、現代の AI 言語モデルの中核をなしています。
これらの原理を理解することで、LLM の能力と限界を把握し、より効果的に活用することができます。

特に重要なポイントは以下の通りです。

- **トークン化**はテキスト処理の基本単位を決定し、言語処理の効率と精度に直接影響します
- **コンテキストウィンドウ**はモデルの「記憶容量」を定義し、文脈理解の範囲を決定します
- **推論プロセス**とその**パラメータ設定**がモデルの出力の質と多様性を左右します

これらの原理が連携することで、人間のような自然な言語理解と生成が実現され、様々な産業でビジネス価値を創出しています。

## 限界と注意点

LLM の強力さは認められる一方で、実際の使用時には以下のような限界と注意点を理解しておくことが重要です。

- **文脈の忘却**: コンテキストウィンドウを超える情報は失われるため、長すぎる履歴には注意が必要です。例えば、100ページの文書のうち前半部分に言及した内容を、後半の質問で忘れてしまうことがあります。

- **事実ではなく尤度ベース**: 推論は「確率的なもっともらしさ」であり、必ずしも真実とは限りません。特に専門的・最新の情報については、常に別の情報源で確認することが望ましいです。

- **トークン課金モデルの制約**: 長い入力ほどコストが増すため、業務用途では使用トークン数の管理が重要です。例えば、会話型エージェントでは過去のやり取りをすべて保持するよりも、要約して保持する方法が経済的です。

- **言語によるトークン効率の差**: 日本語や中国語などは英語と比べて同じ意味を表現するのに多くのトークンを必要とするため、多言語サービス設計時にはこの差を考慮する必要があります。

- **モデルサイズとレイテンシーのトレードオフ**: より大きなモデルやコンテキストウィンドウは通常、処理に時間がかかります。リアルタイム性が求められるアプリケーションでは、このトレードオフを考慮したモデル選択が必要です。

- **特定ドメインへの適応限界**: 一般的な LLM は特定の専門領域（例：最先端の科学研究や特殊な業界用語）において知識や理解に限界があります。専門分野での使用時には、ドメイン特化型の微調整や追加学習が必要になることがあります。

これらを理解することで、LLM の限界を前提とした「現実的な活用戦略」を立てることができます。

## 用語解説

| 用語 | 説明 |
|------|------|
| LLM | Large Language Model（大規模言語モデル）の略。膨大なテキストデータから学習した AI モデル |
| トークン | LLM が処理する最小単位。単語、部分語、文字などに相当。英語では通常、単語は1～2トークン、日本語ではより多くのトークンに分割されることが多い |
| コンテキストウィンドウ | LLM が一度に処理・「記憶」できるトークンの最大数。モデルの「作業記憶」の容量を定義する |
| 推論 | モデルが入力に基づいて出力を生成するプロセス。学習済みパターンから確率的に次のトークンを予測する |
| 温度（Temperature） | 応答のランダム性を制御するパラメータ。高いほど創造的で多様な応答になる。範囲は通常0～1で、0.7が一般的な会話に適している |
| Top-p サンプリング | 確率の高いトークンのみを選択対象とするサンプリング手法。通常0.9〜0.95に設定すると、適度な多様性と一貫性のバランスが取れる |
| 幻覚（Hallucination） | LLM が実際には存在しない情報を事実であるかのように生成する現象。モデルの学習データに含まれない事項について質問されたときに特に発生しやすい |
| トランスフォーマー | 多くの LLM の基礎となるニューラルネットワークアーキテクチャ。自己注意機構（Self-Attention）により、入力シーケンス内の関連性を効率的に学習できる |
