---
title: Attention の内部機構
description: The Internal Mechanism of Attention
icon: Rotate3d
---

import { Mermaid } from "@/components/mdx/mermaid";

## Attention の内部機構：各種 Attention と位置情報の扱い

### 🔑 エグゼクティブサマリー

Attention 機構は現代の自然言語処理 (NLP) とトランスフォーマーモデルの中核技術です。
本ドキュメントでは、Self-Attention、Multi-Head Attention などの各種 Attention メカニズムの内部動作と、位置情報の埋め込み方法について詳細に解説します。

Attention は入力シーケンスの各要素間の関連性を計算し、モデルが長距離依存関係を効率的に学習できるようにする仕組みです。
また、絶対位置エンコーディングと相対位置エンコーディングという位置情報の取り扱い方についても説明します。

### はじめに

#### 本ドキュメントの想定読者

- 機械学習エンジニアやリサーチャー
- トランスフォーマーベースのモデルを理解したいデベロッパー
- 深層学習モデルのアーキテクチャに興味のある学生や研究者

#### 前提知識

本ドキュメントを理解するためには、以下の基礎知識があると望ましいです。

- 基本的な深層学習の概念
- ベクトルと行列の演算
- シーケンスモデリングの基礎

### Attention の基本概念

Attention は、入力シーケンスの各要素間の関連性を計算し、その関連性に基づいて情報を集約するメカニズムです。
2017年に発表された "Attention Is All You Need" 論文で紹介されたトランスフォーマーモデルの中核技術として、現代の自然言語処理の発展に大きく貢献しました。

Attention の主な利点は以下の通りです。

- 長距離依存関係の効率的な学習
- 並列計算による処理速度の向上
- 解釈可能性の向上（どの入力が出力に影響を与えているかの可視化）

### Self-Attention の詳細

#### Self-Attention の基本原理

Self-Attention（自己注意機構）は、同じ入力シーケンス内の各要素がシーケンス内の他の全要素とどのように関連しているかを計算するメカニズムです。

Self-Attention の計算は次の手順で行われます。

1. 入力ベクトル $X$ から Query ($Q$)、Key ($K$)、Value ($V$) の三つの異なる行列を生成
2. Query と Key の行列積を計算しスケーリング（スケーリング係数 $√d_k$ で割る）
3. Softmax 関数を適用してアテンションスコアを正規化
4. 正規化されたスコアと Value 行列の積を計算して最終的な出力を生成

#### Query、Key、Value の役割

Attention メカニズムの核心は、Query ($Q$)、Key ($K$)、Value ($V$) という3つの要素にあります。
これらはそれぞれ独自の重要な役割を持っています。

**Query（クエリ）**：「何を探しているか」を表します
- 現在のトークン（単語）が**どのような情報を必要としているか**を表現するベクトル
- データベースに対する「検索クエリ」と同様の概念で、「私はこの情報が必要です」という問い合わせに相当
- 各トークンは自分の Query ベクトルを使って、他のすべてのトークンとの関連性を計算

**Key（キー）**：「どこに情報があるか」を表します
- 各トークンが**どのような情報を提供できるか**を示すベクトル
- データベースの「インデックス」や「検索キー」のような役割
- Query と Key の類似度（内積）が高いほど、そのトークンは関連性が高いと判断される

**Value（バリュー）**：「情報の内容」を表します
- 各トークンが実際に**提供する情報の内容**を表すベクトル
- データベースでいう「実際のデータ値」に相当
- 最終的に Attention の重みづけに基づいて集約される実際の情報

#### 具体例で理解する

例えば「猫が魚を食べる」という文を処理する場合：

1. 「食べる」というトークンの Query が「主語は何か？」「目的語は何か？」という情報を探しています
2. 「猫」と「魚」のそれぞれの Key が「私は主語です」「私は目的語です」という情報を持っています
3. Query と Key の内積計算により、「食べる」は「猫」と「魚」に高い注意スコアを与えます
4. 「猫」と「魚」の Value が持つ実際の意味情報が、その注意スコアに基づいて「食べる」の新しい表現に集約されます

この機構により、各単語はシーケンス内の関連する他の単語から情報を収集し、文脈を考慮した豊かな表現を獲得することができます。

#### 数学的表現

数学的には以下のように表現できます。

```math
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
```

ここで:
- $Q$ は Query 行列
- $K$ は Key 行列
- $V$ は Value 行列
- $d_k$ は Key ベクトルの次元数

この数式の直感的な意味:
- $QK^T$ は「クエリとキーの類似度」を計算しています。これは「現在の単語が他のどの単語に注目すべきか」を決定します
- $√d_k$ でスケーリングするのは、ベクトルの次元が大きい場合に softmax 関数入力の分散を安定させるためです
- softmax 関数は類似度を [0,1] の確率分布に変換し、最も関連性の高い単語に高い重みを与えます
- 最後に重み付けされた Value 行列との積により、関連性の高い単語からより多くの情報を取り込んだ新しい表現が生成されます

具体例として、「私は猫が好きです」という文では、「猫」という単語を処理する際に、関連性の高い「好き」という単語に対する Attention スコアが高くなり、
その意味的関連性を捉えることができます。

<Mermaid chart={`
graph LR
    X[入力] --> Q[Query]
    X --> K[Key]
    X --> V[Value]
    Q & K --> S[QK^T 行列積]
    S --> Scale[スケーリング]
    Scale --> SM[Softmax]
    SM & V --> A[Attention出力]

    style X fill:#87CEFA,stroke:#0047AB,color:#000
    style Q fill:#90EE90,stroke:#006400,color:#000
    style K fill:#90EE90,stroke:#006400,color:#000
    style V fill:#FFD700,stroke:#B8860B,color:#000
    style A fill:#FF6347,stroke:#8B0000,color:#000
`} />

#### 具体的な計算例

簡単な例で Self-Attention の計算を見てみましょう。入力として単純な3つの単語からなるシーケンスを考えます。

1. 各単語が 4 次元の埋め込みベクトルで表現されていると仮定
2. 入力行列 $X$ の形状: [3単語 × 4次元]
3. 重み行列 $W_Q$, $W_K$, $W_V$ を用いて $Q$, $K$, $V$ を生成
4. $Q$, $K$, $V$ の形状はすべて [3単語 × 4次元]
5. $Q$ と $K$ の行列積: [3 × 4] · [4 × 3] = [3 × 3] の行列
6. スケーリング: 行列の各要素を $√4 = 2$ で割る
7. Softmax 適用: 各行が確率分布になるよう変換
8. $V$ との行列積: [3 × 3] · [3 × 4] = [3 × 4] の最終出力行列

実際の数値を用いた計算例を示します。

仮に「I like cats」という3単語のシーケンスがあり、各単語の埋め込みベクトルが以下の通りとします：

$$
\begin{array}{rcl}
X &=&
\left[
\begin{array}{cccc}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0
\end{array}
\right]
\begin{array}{l}
\text{"I"} \\
\text{"like"} \\
\text{"cats"}
\end{array}
\end{array}
$$

単純化のため、重み行列を単位行列とすると、$Q = K = V = X$ となります。

**ステップ 1**:

$QK^T$ の計算
$$
QK^T =
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0
\end{bmatrix}
\cdot
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 \\
0 & 0 & 0
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
$$

**ステップ 2**:

スケーリング（√4 = 2 で割る）

```math
\frac{QK^T}{\sqrt{d_k}} =
\begin{bmatrix}
0.5 & 0 & 0 \\
0 & 0.5 & 0 \\
0 & 0 & 0.5
\end{bmatrix}
```

**ステップ 3**:

Softmax の適用

```math
\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) =
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1
\end{bmatrix}
```

<Callout type="info">
この例では単位行列なので Softmax 後も変化しませんが、一般的には各行が確率分布になります
</Callout>

**ステップ 4**:

$V$ との積
$$
\text{Attention} =
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0
\end{bmatrix}
\cdot
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0
\end{bmatrix}
$$

この単純な例では元の入力と同じ結果になりますが、実際の場合は重み行列が学習され、各単語間の関連性が Attention の結果に反映されます。
例えば、「like」と「cats」の間に強い関連性がある場合、Attention 行列は対角成分以外にも大きな値を持ち、出力ベクトルには他の単語からの情報が混合されます。

#### より実際的な Softmax 結果の例

より実際的なシナリオでは、単語間に様々な関連性があります。
例えば「I like cats」という文では、「like」は「I」（主語）と「cats」（目的語）の両方と関連しています。
このような場合の Attention 計算を簡略化した例を見てみましょう。

仮に $QK^T$ の計算結果（スケーリング済み）が以下のような値になったとします：

```math
\begin{array}{rcl}
\frac{QK^T}{\sqrt{d_k}} &=&
\left[
\begin{array}{ccc}
0.8 & 0.3 & 0.1 \\
0.4 & 0.5 & 0.7 \\
0.2 & 0.6 & 0.9
\end{array}
\right]
\begin{array}{l}
\text{"I" の関連性スコア} \\
\text{"like" の関連性スコア} \\
\text{"cats" の関連性スコア}
\end{array}
\end{array}
```

Softmax を適用すると：

```math
\begin{array}{rcl}
\text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) &=&
\left[
\begin{array}{ccc}
0.67 & 0.27 & 0.06 \\
0.25 & 0.28 & 0.47 \\
0.12 & 0.36 & 0.52
\end{array}
\right]
\begin{array}{l}
\text{"I" は自分自身に最も注目} \\
\text{"like" は "cats" に最も注目} \\
\text{"cats" は自分自身に最も注目するが "like" にも注目}
\end{array}
\end{array}
```

この Attention 行列から各単語の解釈を見ると：

「I」は主に自分自身に注目（67%）していますが、「like」にも一定の注意（27%）を払っています
「like」は目的語の「cats」に最も注目（47%）し、次に自身（28%）、そして主語「I」（25%）にも注目しています
「cats」は自身に最も注目（52%）していますが、それを修飾する動詞「like」にも大きく注目（36%）しています

このような Attention パターンが V 行列（各単語のコンテンツ）と掛け合わされることで、各単語の文脈を考慮した新しい表現が生成されます。
この例では、「like」は主語と目的語の情報を取り込んだ表現に、「cats」は「like」の情報を部分的に取り込んだ表現になります。

### Multi-Head Attention の詳細

#### Multi-Head Attention の仕組み

Multi-Head Attention は、異なる「ヘッド」で並行して複数の Self-Attention を実行し、その結果を結合するメカニズムです。
これにより、異なる観点から入力の関係性を学習できます。

Multi-Head Attention の処理手順は以下の通りです。

1. 入力を h 個の異なるヘッドに分割
2. 各ヘッドで独立に Self-Attention を計算
3. 各ヘッドの出力を結合
4. 結合された出力に線形変換を適用

数学的には以下のように表されます。

$ \text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O $

ここで:

$ \text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V) $

<Mermaid chart={`
graph TD
X[入力テンソル] -->|次元分割| QKV1["Q/K/V for Head 1 (部分特徴空間1)"]
X -->|次元分割| QKV2["Q/K/V for Head 2 (部分特徴空間2)"]
X -->|次元分割| QKVh["Q/K/V for Head h (部分特徴空間h)"]
QKV1 -->|注意計算| H1["Head 1 Attention (視点1の関係性)"]
QKV2 -->|注意計算| H2["Head 2 Attention (視点2の関係性)"]
QKVh -->|注意計算| Hh["Head h Attention (視点hの関係性)"]

H1 & H2 & Hh -->|結果統合| C["Concat (多視点の結合)"]
C -->|次元調整| WO["線形変換 W^O (出力次元の統一)"]
WO --> Final["最終出力 (多視点の総合表現)"]

style X fill:#87CEFA,stroke:#0047AB,color:#000
style QKV1 fill:#D8BFD8,stroke:#800080,color:#000
style QKV2 fill:#D8BFD8,stroke:#800080,color:#000
style QKVh fill:#D8BFD8,stroke:#800080,color:#000
style H1 fill:#90EE90,stroke:#006400,color:#000
style H2 fill:#90EE90,stroke:#006400,color:#000
style Hh fill:#90EE90,stroke:#006400,color:#000
style Final fill:#FF6347,stroke:#8B0000,color:#000
`} />

<Callout title="Multi-Head Attention の凡例">
- **$Q/K/V$ for Head**: 各ヘッド用の特徴抽出
- **Head Attention**: 注意機構の適用（各ヘッドが異なる関係性に注目）
- **Concat**: 複数の視点からの結果の結合
- **線形変換 $W^O$**: 出力の次元調整と情報の統合
</Callout>

#### Multi-Head Attention の利点

Multi-Head Attention には以下の利点があります。

- 異なる表現サブ空間からの情報を並行して学習
- 様々な種類の関係や依存関係を同時に捉える能力
- モデルの表現力の向上

##### 表現の多様性の具体例

Multi-Head Attention の大きな強みは、各ヘッドが異なる種類の関係性に注目できることです。以下は実際の言語処理でよく観察される例です：

- **ヘッド1（文法的関係）**：
  「彼女は本を読んでいる」という文において、「彼女」と「読んでいる」の間の主語-動詞関係を捉える。

- **ヘッド2（修飾関係）**：
  「赤い大きな本」において、「赤い」「大きな」と「本」の間の形容詞-名詞関係に注目する。

- **ヘッド3（共参照関係）**：
  「田中さんは会議に出席した。彼は意見を述べた。」で、「田中さん」と「彼」が同一人物であることを認識する。

- **ヘッド4（意味的類似性）**：
  「コンピュータ」と「パソコン」のような同義語・類義語の関係を捉える。

- **ヘッド5（時間的順序）**：
  「まず〜して、次に〜する」といった順序関係に注目する。

これらの多様な「視点」を並行して学習することで、テキストの複雑な構造と意味を包括的に理解できるようになります。これは単一の Attention メカニズムでは難しい成果です。

### 位置情報の扱い

トランスフォーマーモデルでは、位置情報を明示的に組み込む必要があります。これは Self-Attention が入力シーケンスの順序情報を持たないためです。
位置情報を扱う主な方法は以下の二つです。本章では「絶対位置エンコーディング」について詳しく解説し、次の章で「相対位置エンコーディング」について説明します。

#### 位置情報がないとどうなるか？

位置情報を組み込まずに Self-Attention だけを使用した場合、モデルは入力シーケンスの順序を認識できません。例えば：

- 「猫が犬を追いかけた」と「犬が猫を追いかけた」という文は、単語の集合としては同じですが、順序が違うため意味が全く異なります。
- 単純な Attention では、これらの文は同一に扱われてしまい、「誰が誰を追いかけたのか」という重要な情報が失われてしまいます。

実験的に示すと、位置情報なしのモデルでは：
1. 「猫が犬を追いかけた」→ [猫, が, 犬, を, 追いかけた] → [動作主=?, 対象=?]
2. 「犬が猫を追いかけた」→ [犬, が, 猫, を, 追いかけた] → [動作主=?, 対象=?]

両方の文に対して同じ表現を学習してしまうため、正確な文の理解ができません。

また、「彼女は彼を愛している」と「彼は彼女を愛している」など、代名詞の参照関係を理解する上でも順序情報は不可欠です。
このように、文の意味や構文解析において位置情報は決定的に重要な役割を果たします。

#### 絶対位置エンコーディング

絶対位置エンコーディングは、入力シーケンスの各位置に固有の位置ベクトルを割り当てる方法です。

オリジナルのトランスフォーマーモデルでは、正弦・余弦関数を使用した位置エンコーディングが使われています。

```math
PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)
```
```math
PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)
```

ここで:
- $pos$ はトークンの位置
- $i$ は埋め込みの次元インデックス
- $d_{model}$ は埋め込みの次元数

この方法の特徴は以下の通りです。

- 固定的な位置エンコーディング（学習不要）
- 任意の長さのシーケンスに対応可能
- 相対的な位置関係を簡単に計算可能

<Mermaid chart={`
graph LR
X[単語埋め込み] --> Add["加算 (+)"]
PE[位置エンコーディング] --> Add
Add --> Out[位置情報付き埋め込み]
style X fill:#87CEFA,stroke:#0047AB,color:#000
style PE fill:#FFD700,stroke:#B8860B,color:#000
style Out fill:#FF6347,stroke:#8B0000,color:#000
style Add fill:#FFFFFF,stroke:#000000,color:#000
`} />
<Mermaid chart={`
graph LR
PE0[位置0の埋め込み] --> PE[位置エンコーディング]
PE1[位置1の埋め込み] --> PE
PE2[位置2の埋め込み] --> PE
T0[トークン0] --> X[単語埋め込み]
T1[トークン1] --> X
T2[トークン2] --> X

style PE0 fill:#D8BFD8,stroke:#800080,color:#000
style PE1 fill:#D8BFD8,stroke:#800080,color:#000
style PE2 fill:#D8BFD8,stroke:#800080,color:#000
style T0 fill:#A0E0A0,stroke:#006000,color:#000
style T1 fill:#A0E0A0,stroke:#006000,color:#000
style T2 fill:#A0E0A0,stroke:#006000,color:#000
style X fill:#87CEFA,stroke:#0047AB,color:#000
style PE fill:#FFD700,stroke:#B8860B,color:#000
`} />

<Callout title="位置エンコーディングの凡例">
- **単語埋め込み**: 単語の意味的特徴を表現するベクトル
- **位置エンコーディング**: 単語の順序情報を表現するベクトル
- **加算 (+)**: 単純な要素ごとの加算による情報の統合
- **位置情報付き埋め込み**: 意味と順序の両方を含む最終表現
</Callout>

#### $Sin/Cos$ 位置エンコーディングの波形パターン

トランスフォーマーの位置エンコーディングが正弦・余弦関数を使用する理由の一つは、異なる次元で異なる周期を持つことで、位置情報を効果的に表現できるからです。
以下はその波形パターンを示しています。

<Mermaid chart={`
graph TD
subgraph "位置エンコーディングの波形パターン"
direction LR
subgraph "次元0 (高周波)"
Dim0[" "] --- D0Wave[sin/cos波 - 周期短]
end

subgraph "次元2 (中周波)"
Dim2[" "] --- D2Wave[sin/cos波 - 周期中]
end

subgraph "次元4 (低周波)"
Dim4[" "] --- D4Wave[sin/cos波 - 周期長]
end

subgraph "合成パターン"
Pos0[位置0] --- Pattern0["一意なパターン"]
Pos1[位置1] --- Pattern1["一意なパターン"]
Pos2[位置2] --- Pattern2["一意なパターン"]
end

D0Wave & D2Wave & D4Wave --> Pattern0 & Pattern1 & Pattern2

style Dim0 fill:#FFFFFF,stroke:#FFFFFF
style Dim2 fill:#FFFFFF,stroke:#FFFFFF
style Dim4 fill:#FFFFFF,stroke:#FFFFFF
style D0Wave fill:#FFD700,stroke:#B8860B,color:#000
style D2Wave fill:#FFD700,stroke:#B8860B,color:#000
style D4Wave fill:#FFD700,stroke:#B8860B,color:#000
style Pattern0 fill:#FF6347,stroke:#8B0000,color:#000
style Pattern1 fill:#FF6347,stroke:#8B0000,color:#000
style Pattern2 fill:#FF6347,stroke:#8B0000,color:#000
end
`} />

<Callout title="Sin/Cos 波形パターンの凡例">
- **高周波波形**: 位置の細かい違いを捉える（近接単語の区別）
- **低周波波形**: 遠い位置の関係を捉える（長距離依存関係）
- **合成パターン**: 各位置の一意な表現（位置の識別子）
</Callout>

この図が示すように、各次元（埋め込みベクトルの各要素）は異なる周波数の正弦波または余弦波で表現されます。
次元が上がるにつれて波の周期は長くなります（「正弦波／余弦波の周波数を制御するためのスケーリング係数」の指数関数的増加）。

<Callout title="正弦波／余弦波の周波数を制御するためのスケーリング係数">
```math
10000^{\frac{2i}{d_{\text{model}}}}
```
</Callout>

これにより、各位置は一意なパターンを持ち、相対的な位置関係も保持されます。
例えば、位置 $pos$ と $pos+k$ の内積は $k$ のみに依存し、これが相対位置の学習を容易にします。

#### 相対位置エンコーディング

相対位置エンコーディングは、トークン間の相対的な距離に基づいて位置情報を埋め込む方法です。

代表的な手法として Shaw らの「Self-Attention with Relative Position Representations」があります。
この方法では、Self-Attention の計算に相対位置情報を直接組み込みます。

```math
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + R\right) V
```

ここで $R$ は相対位置行列です。この相対位置行列 $R$ は以下のように定義されます：

```math
R_{i,j} = a_{i-j}
```

ここで:
- $R_{i,j}$ は位置 $i$ のトークンと位置 $j$ のトークンの間の相対位置関係を表す値
- $a_{i-j}$ は相対距離 ($i-j$) に対応する学習可能なパラメータ

実際の実装では、通常は相対位置のクリッピング（例えば $-k$ から $k$ までの範囲に制限）を行い、学習すべきパラメータ数を削減します。
この方法により、モデルはトークン間の絶対位置ではなく、相対的な距離に基づいて Attention を計算できるようになります。

さらに詳細な実装では、相対位置エンコーディングは以下のように拡張されることもあります。

```math
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T + Q \cdot R_K + R_Q \cdot K^T}{\sqrt{d_k}}\right) V
```

ここで:
- $R_K$ は位置キーの相対表現を表す行列
- $R_Q$ は位置クエリの相対表現を表す行列

具体的な例として、5トークンの文で相対位置行列を考えてみましょう：

$$
R = \begin{bmatrix}
a_0 & a_{-1} & a_{-2} & a_{-3} & a_{-4} \\
a_1 & a_0 & a_{-1} & a_{-2} & a_{-3} \\
a_2 & a_1 & a_0 & a_{-1} & a_{-2} \\
a_3 & a_2 & a_1 & a_0 & a_{-1} \\
a_4 & a_3 & a_2 & a_1 & a_0
\end{bmatrix}
$$

この行列の各要素 $R_{i,j}$ は、位置 $i$ のトークンから見た位置 $j$ のトークンの相対位置を表しています。
例えば、$R_{2,1} = a_1$ は、2番目のトークンから見て1番目のトークンは「1つ前」にあることを意味します。

相対位置エンコーディングの利点は、シーケンスの長さに依存せず、同じ相対距離のトークンペアは同じ位置情報を共有できることです。
これにより、モデルが学習していない長さのシーケンスにも容易に一般化できます。

この方法の特徴は以下の通りです。

- より柔軟な位置関係の学習が可能
- 長いシーケンスでより効果的
- 計算コストが若干高い

相対位置エンコーディングの追加的なメリットとしては以下が挙げられます。

- **未知の長さのシーケンスへの一般化能力**: 絶対位置エンコーディングでは、学習時に見たことのない長さのシーケンスに対応するのが難しい場合がありますが、相対位置では「2つ前の単語」といった相対的な関係のみを扱うため、より柔軟に対応できます。
- **文の中心位置に依存しない表現**: 例えば「AとBの間にはCがある」という関係は、文のどの位置に出現しても同じ相対関係として扱われるため、位置に依存しない言語的パターンの学習に優れています。
- **トランスレーション不変性**: 文全体をシフトさせても（例：文の先頭に単語を追加しても）、相対的な関係は保持されるため、文の構造的理解がより堅牢になります。
- **言語横断的な転移学習の容易さ**: 異なる言語でも、文法的な相対関係（例：形容詞と名詞の関係）は類似していることが多く、相対位置表現はこうした言語間の転移に有利です。

### 発展的な Attention 機構

最近の研究では、基本的な Attention 機構を拡張した様々なバリエーションが提案されています。

#### Sparse Attention

Sparse Attention は、全てのトークンペア間の関係を計算する代わりに、一部のペアのみに注目することで計算効率を向上させる手法です。

主な特徴は以下の通りです。

- 計算量を $O(n^2)$ から $O(n \log n)$ または $O(n)$ に削減
- 長いシーケンスでの効率的な処理を実現
- Longformer や BigBird などのモデルで採用

応用例と解決する問題:
- **Longformer**: 長文書処理のために設計され、ローカルな Attention とグローバルな Attention を組み合わせています。科学論文や書籍全体などの長いテキストを効率的に処理できるため、要約や質問応答タスクで優れた性能を発揮します。
- **BigBird**: Longformer と同様に長いシーケンスを扱いますが、ランダムな接続も導入しています。これにより、ゲノムシーケンスの分析など、非常に長いシーケンスデータの処理が可能になりました。

#### Efficient Attention

計算効率を向上させるための様々な Attention 変種があります。

主な例としては以下があります。

- **Reformer**: ハッシュベースの近似を使用して似たベクトルを効率的に見つけ出します。これにより非常に長いテキスト（数万トークン）でも効率的に処理でき、特に限られたメモリ環境での言語モデリングに有効です。
- **Performer**: カーネル法を用いた近似によって、明示的な Attention 行列を計算せずに同等の結果を得ます。特に画像処理やタンパク質配列モデリングなどの高次元データ処理で威力を発揮します。
- **Linformer**: 低ランク行列近似を使用して Attention の計算複雑性を削減します。ソーシャルメディアの投稿分析やニュース記事の分類など、中程度の長さのテキスト処理タスクに適しています。

### まとめ

本ドキュメントでは、Attention 機構の内部動作とその応用について説明しました。
Attention の基本概念から始まり、Self-Attention、Multi-Head Attention の詳細な仕組みと、位置情報の扱い方について解説しました。

現代の自然言語処理における Attention の重要性は非常に高く、様々な拡張や改良も続けられています。
効率的な Attention 機構の開発は、より大規模かつ効果的な言語モデルの実現に繋がっています。

#### 絶対位置エンコーディングと相対位置エンコーディングの比較

以下の表は、絶対位置エンコーディングと相対位置エンコーディングの主な特徴を比較したものです。

| 特徴 | 絶対位置エンコーディング | 相対位置エンコーディング |
|------|--------------------------|--------------------------|
| 実装の複雑さ | 比較的簡単 | 複雑（行列操作・学習要素あり） |
| 学習可能性 | 基本的に固定（sin/cos） | 学習可能なパラメータあり |
| メモリ効率 | 高い（固定長ベクトル） | やや低い（相対位置行列が必要） |
| 一般化性能 | 任意長対応可（sin/cos） | より柔軟に順序関係を学習 |
| 長距離依存性 | 制限あり | より効果的に捉えられる |
| 主な用途 | 初期のTransformer（BERT等） | Transformer-XL, T5, DeBERTaなど |
| 計算コスト | 低い | 高い |
| トークン数制限 | 学習時の最大長に制約 | より柔軟に対応可能 |

両方のアプローチにはそれぞれ長所と短所があり、タスクや計算リソースに応じて適切な方法を選択することが重要です。
最近のモデルでは、これらを組み合わせたハイブリッドアプローチも採用されています。

#### 位置エンコーディングの詳細比較

| 項目 | 絶対位置エンコーディング | 相対位置エンコーディング |
|------|--------------------------|--------------------------|
| 概要 | 各位置に一意のベクトルを割当 | 単語間の距離に基づいて重み計算 |
| モデル依存性 | トークン位置に固定 | Attention に組み込まれる |
| 位置参照方法 | グローバル（原点からの距離） | ローカル（トークン間の距離） |
| 計算例 | PE(pos) を埋め込みに加算 | QK^T に相対位置バイアス R を加算 |
| 長所 | 実装が容易、計算コスト低 | 長文での性能が高い、位置転移に強い |
| 短所 | 学習済み長さ以上の入力に弱い | 計算量が増加、実装が複雑 |
| 代表的モデル | BERT, GPT-2 | Transformer-XL, T5, DeBERTa |
| 未学習長への対応 | 外挿性に制限あり | より良い一般化能力 |

### 用語解説

| 用語 | 説明 |
|------|------|
| Attention | 入力シーケンスの各要素間の関連性を計算し情報を集約するメカニズム |
| Self-Attention | 同一シーケンス内の各要素がシーケンス内の他の全要素とどのように関連しているかを計算するメカニズム |
| Query ($Q$) | Attention 計算において、「何を探しているか」を表現するベクトル |
| Key ($K$) | Attention 計算において、「何に関する情報か」を表現するベクトル |
| Value ($V$) | Attention 計算において、「実際の情報内容」を表現するベクトル |
| Multi-Head Attention | 複数の並行した Attention を計算し結合するメカニズム |
| 位置エンコーディング | シーケンス内のトークンの位置情報をモデルに組み込む方法 |
| 絶対位置エンコーディング | 各トークンの絶対的な位置に基づく情報を組み込む方法 |
| 相対位置エンコーディング | トークン間の相対的な距離に基づく情報を組み込む方法 |
| Sparse Attention | 一部のトークンペアのみに注目することで計算効率を向上させる手法 |
