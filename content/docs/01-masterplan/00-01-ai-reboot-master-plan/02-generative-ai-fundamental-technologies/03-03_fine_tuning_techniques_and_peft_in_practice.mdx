---
title: ファインチューニング技術と PEFT の実践
description: Fine-tuning Techniques and PEFT in Practice
icon: Activity
---

import { Mermaid } from "@/components/mdx/mermaid";

## ファインチューニング技術と PEFT の実践

### 🔑 エグゼクティブサマリー

本ドキュメントでは、大規模言語モデル（LLM）のファインチューニング技術、特に Parameter-Efficient Fine-Tuning（PEFT）手法について解説します。
従来の全パラメータのファインチューニングに比べ、PEFT は計算リソースを大幅に削減しながら効果的にモデルをカスタマイズできる手法です。
LoRA、Adapter、Prefix Tuning などの主要な PEFT 手法の仕組み、実装方法、利点と制約について実践的な観点から説明します。

#### 想定読者

- 機械学習エンジニア
- NLP 研究者
- AI アプリケーション開発者
- 計算リソースを効率的に使いたい組織の技術担当者

#### 対象システム規模

- 中小規模の GPU 環境（単一 GPU から数台の GPU クラスター）
- クラウドベースの AI インフラストラクチャ
- 限られた計算リソースでの大規模モデル応用

### ファインチューニングの概要

ファインチューニングとは、事前学習済みの大規模モデルを特定のタスクやドメインに適応させるプロセスです。
従来の方法では、モデルの全パラメータを更新する必要がありましたが、これには以下の課題があります。

- 膨大な計算リソースが必要
- 大量のメモリを消費
- 過学習のリスクが高い
- 保存に大きなストレージが必要

これらの課題を解決するために登場したのが PEFT（Parameter-Efficient Fine-Tuning）手法です。

### PEFT（Parameter-Efficient Fine-Tuning）とは

PEFT は、モデルの一部のパラメータのみを更新することで、効率的にファインチューニングを行う手法の総称です。
PEFT の主な利点は以下の通りです。

- 計算コストの大幅削減（最大 99% 以上）
- メモリ使用量の削減
- 過学習リスクの低減
- 元のモデル性能を維持しつつ特定タスクへの適応が可能
- 保存・デプロイが容易

#### PEFT 手法の全体マップ

以下の図は、主要な PEFT 手法の分類と関係性を示しています。

<Mermaid chart={`
graph TB
    A["PEFT (Parameter-Efficient Fine-Tuning)"] --> B[Low-Rank系]
    A --> C[Prompt系]
    A --> D[Adapter系]
    B --> B1[LoRA]
    C --> C1[Prefix Tuning]
    C --> C2[P-Tuning]
    D --> D1[Adapter Tuning]
    style A fill:#87CEFA,stroke:#0047AB,color:#000
    style B fill:#90EE90,stroke:#006400,color:#000
    style C fill:#FFD700,stroke:#B8860B,color:#000
    style D fill:#FF6347,stroke:#8B0000,color:#000
`} />

**図の説明**:
- **Low-Rank系**: 重み行列を低ランク近似で更新する手法。計算効率が高く、パラメータ数を大幅に削減します。
- **Prompt系**: モデルの入力や内部状態に学習可能なプロンプトを追加する手法。入力表現を変更することで挙動を制御します。
- **Adapter系**: モデルアーキテクチャに小さなニューラルネットワーク層を追加する手法。モジュール性が高く、複数タスクに対応しやすいです。

### 主要な PEFT 手法

#### LoRA（Low-Rank Adaptation）

LoRA は、大規模言語モデルの効率的なファインチューニングのために開発された手法で、行列の低ランク分解を利用します。

##### 仕組み

LoRA の基本的な考え方は以下の通りです。

<Mermaid chart={`
graph TD
    A[元の重み行列 W] --> B[凍結]
    A --> C[低ランク行列による近似]
    C --> D[A: 低ランク行列]
    C --> E[B: 低ランク行列]
    D --> F[A × B = ΔW]
    E --> F
    B --> G[W + ΔW]
    F --> G
    G --> H[最終的な適応された重み]

    style A fill:#87CEFA,stroke:#0047AB,color:#000
    style B fill:#90EE90,stroke:#006400,color:#000
    style C fill:#FFD700,stroke:#B8860B,color:#000
    style F fill:#FF6347,stroke:#8B0000,color:#000
    style H fill:#DDA0DD,stroke:#8B008B,color:#000
`} />

**図の説明**:
この図は LoRA の仕組みを示しています。
元の大きな重み行列 W を凍結（更新しない）状態で保持しながら、小さな低ランク行列 A と B の積（A × B = ΔW）を使って効率的に更新を行います。
最終的には元の重み W に更新分 ΔW を加えることで、少ないパラメータでモデルの振る舞いを調整できます。
低ランク行列のランク r が小さいほどパラメータ効率が高まりますが、表現力とのバランスが重要です。

1. 元の重み行列 W を凍結（更新しない）
2. 低ランク行列 A と B を導入（r « min(d,k) となるランク r）
3. 更新は ΔW（デルタ W）= A × B のみで行う
4. 最終出力では凍結された W に ΔW を加えた重みを使用

##### 実装例

以下は LoRA を Hugging Face の PEFT ライブラリを使って実装する例です。

```python title="LoRA の実装例"
from peft import get_peft_model, LoraConfig, TaskType

# LoRA の設定
lora_config = LoraConfig(
    r=8,                       # ランク：低ランク行列の次元数（小さいほどパラメータ効率が良いが、表現力が制限される）
    lora_alpha=16,             # スケーリング係数：大きいほど元の重みに対する更新の影響が強くなる
    lora_dropout=0.1,          # ドロップアウト率：過学習防止のためのレギュラリゼーション
    bias="none",               # バイアスパラメータの処理方法（"none"は更新しない）
    task_type=TaskType.CAUSAL_LM,  # タスクの種類（ここでは因果言語モデル用）
    target_modules=["q_proj", "v_proj"]  # LoRA を適用するモジュール（クエリと値の投影層）
)

# ベースモデルを LoRA 化
peft_model = get_peft_model(base_model, lora_config)

# 訓練可能なパラメータ数の確認
trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in peft_model.parameters())
print(f"訓練可能パラメータ: {trainable_params} ({100 * trainable_params / total_params:.2f}%)")
```

このコード例の解説：

1. `r=8` - 低ランク行列の次元数を設定。この値が小さいほどパラメータ数は少なくなりますが、モデルの表現力も制限されます。タスクの複雑さに応じて調整します。
2. `lora_alpha=16` - スケーリング係数。ΔW の影響度を調整します。通常は r の 1〜3 倍程度の値を設定します。
3. `lora_dropout=0.1` - LoRA レイヤーに適用するドロップアウト率。過学習防止に役立ちます。
4. `bias="none"` - バイアスパラメータを更新するかどうかを指定。メモリ効率を高めるため、多くの場合は更新しません。
5. `task_type=TaskType.CAUSAL_LM` - タスクの種類を指定。これによって適切なレイヤーが選択されます。
6. `target_modules=["q_proj", "v_proj"]` - LoRA を適用するモジュールを指定。一般的には注意機構のクエリと値の投影層に適用すると効果的です。

最後のコードブロックでは、訓練可能なパラメータの数と全体に対する割合を計算しています。LoRA の効果を確認するための重要な指標です。

##### 利点と制約

**利点:**
- パラメータ効率が非常に高い（通常は全体の 0.1% 未満のパラメータのみを更新）
- メモリ効率が良く、単一 GPU でも大規模モデルにファインチューニング可能
- 元のモデル構造を変更せず、必要に応じて複数の LoRA アダプタを切り替え可能

**制約:**
- 全てのレイヤーに適用すると効率が下がる場合がある
- ハイパーパラメータ（r, alpha など）の選択が重要
- 極端に少ないデータでは効果が限定的な場合も

##### 適用事例

LoRA は以下のようなユースケースで特に効果的です。

- **特化型の言語生成システム**: 大規模言語モデルを特定ドメイン（医療、法律、金融など）向けに特化させる場合に効果的です。例えば、医療用語に特化した診断支援システムや法律文書の自動生成ツールでの活用事例があります。
- **低リソース環境での実装**: 限られた計算リソースしか利用できない環境（例：単一 GPU のワークステーション）でも大規模モデルを効率的にカスタマイズできます。例えば、スタートアップや小規模研究機関での応用例が多数あります。
- **複数の特化モデルの管理**: 単一のベースモデルに対して複数の LoRA アダプターを訓練・管理することで、用途に応じてモデルの挙動を切り替えられます。例えば、コンテンツ生成プラットフォームで異なるスタイルやトピックに特化したモデルを効率的に提供する事例があります。

#### Adapter Tuning

Adapter Tuning は、モデルのアーキテクチャに小さなニューラルネットワーク（アダプター）を挿入する手法です。

##### 仕組み

<Mermaid chart={`
graph TD
    A[トランスフォーマーレイヤー] --> B[Self-Attention]
    B --> C[アダプターモジュール]
    C --> D[Feed-Forward]
    D --> E[アダプターモジュール]
    E --> F[次のレイヤー]

    C1[アダプターの内部構造] --> C2[ダウンプロジェクション]
    C2 --> C3[非線形活性化]
    C3 --> C4[アッププロジェクション]
    C4 --> C5[残差接続]

    style A fill:#87CEFA,stroke:#0047AB,color:#000
    style C fill:#FFD700,stroke:#B8860B,color:#000
    style E fill:#FFD700,stroke:#B8860B,color:#000
    style C1 fill:#90EE90,stroke:#006400,color:#000
`} />

**図の説明**:
この図は Adapter Tuning の仕組みを示しています。
左側はトランスフォーマーレイヤーの構造で、Self-Attention と Feed-Forward の各サブレイヤーの後にアダプターモジュールを挿入しています。
右側はアダプターモジュール自体の内部構造を示しており、入力をまず低次元に圧縮（ダウンプロジェクション）し、非線形活性化を適用した後、元の次元に戻す（アッププロジェクション）構造になっています。
最後に残差接続で入力を足し合わせます。このボトルネック構造により、追加するパラメータを効率的に削減しています。

1. 元のトランスフォーマーレイヤーの各サブレイヤー後にアダプターモジュールを挿入
2. アダプターは以下の構造を持つ:
   - 入力 -> ダウンプロジェクション -> 活性化 -> アッププロジェクション -> 残差接続 -> 出力
3. 元のモデルパラメータは凍結し、アダプターのパラメータのみを訓練

##### 実装例

以下は Adapter を Hugging Face の PEFT ライブラリを使って実装する例です。

```python title="Adapter の実装例"
from peft import AdapterConfig, get_peft_model

# アダプター設定
adapter_config = AdapterConfig(
    hidden_size=768,           # 隠れ層のサイズ：モデルの隠れ層の次元数と一致させる
    adapter_size=64,           # アダプターの中間層サイズ：小さいほど効率的だが表現力が制限される
    adapter_activation="relu", # 活性化関数：非線形性を追加するための関数
    adapter_layers=["attention", "mlp"]  # アダプターを挿入するレイヤー：注意機構と FFN の両方に適用
)

# ベースモデルにアダプターを適用
peft_model = get_peft_model(base_model, adapter_config)

# 訓練可能なパラメータの確認
trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in peft_model.parameters())
print(f"訓練可能パラメータ: {trainable_params} ({100 * trainable_params / total_params:.2f}%)")
```

このコード例の解説：

1. `hidden_size=768` - ベースモデルの隠れ層の次元数を指定します。例えば BERT-base では 768、GPT-2 では 768 や 1024 などです。
2. `adapter_size=64` - アダプターのボトルネック次元数を指定します。この値が小さいほどパラメータ効率が良くなりますが、表現力も制限されます。
3. `adapter_activation="relu"` - アダプター内の活性化関数を指定します。ReLU や GELU が一般的です。
4. `adapter_layers=["attention", "mlp"]` - アダプターを挿入するレイヤーを指定します。通常は注意機構と MLP（Feed-Forward Network）の両方に追加します。

ベースモデルにアダプターを適用した後、訓練可能なパラメータ数を確認することで、全パラメータの何パーセントを更新するかが分かります。一般的には 1〜5% 程度になります。

##### 適用事例

Adapter は以下のようなユースケースで特に効果的です。

- **マルチタスク学習**: 共通のベースモデルに対して、タスクごとに異なるアダプターを訓練し、切り替えて使用できます。例えば、単一の BERT モデルに異なる言語の翻訳タスク用のアダプターを追加する事例があります。
- **リソース制約のある環境**: 計算リソースが限られている状況でも、大規模モデルを効率的にファインチューニングできます。例えば、医療や法律など専門分野向けの言語モデルのカスタマイズに使用されています。
- **継続的な学習**: 新しいデータやタスクが追加されるたびに、新しいアダプターを訓練するだけで対応できます。例えば、ニュース記事の分類システムで、新しいカテゴリが追加されるたびにそのカテゴリ用のアダプターを追加する方法が採用されています。

##### 利点と制約

**利点:**
- モジュール性が高く、複数のタスク用アダプターを組み合わせ可能
- 元のモデル構造への影響が少ない
- 比較的安定した訓練プロセス

**制約:**
- LoRA よりもわずかに多くのパラメータを追加
- 推論時のレイテンシーがわずかに増加
- モデルアーキテクチャによっては実装が複雑になる場合がある

#### Prefix Tuning

Prefix Tuning は、モデルの入力に学習可能なプレフィックスを追加する手法です。

##### 仕組み

<Mermaid chart={`
graph TD
    A[入力シーケンス] --> B[プレフィックスパラメータ]
    A --> C[埋め込み層]
    B --> D[連結]
    C --> D
    D --> E[トランスフォーマーレイヤー]
    E --> F[出力]

    style A fill:#87CEFA,stroke:#0047AB,color:#000
    style B fill:#FFD700,stroke:#B8860B,color:#000
    style E fill:#90EE90,stroke:#006400,color:#000
`} />

**図の説明**:
この図は Prefix Tuning の仕組みを示しています。入力シーケンスの埋め込み表現に加えて、学習可能なプレフィックスパラメータをモデルに導入します。
これらのプレフィックスは各トランスフォーマーレイヤーの自己注意層の入力に追加され、モデルの挙動をコントロールします。
入力テキスト自体は変更せず、モデルの内部表現のみを操作するため、柔軟性が高いのが特徴です。
プレフィックスパラメータは通常、小さなMLP（多層パーセプトロン）を通じて生成され、パラメータ効率を高めています。

1. 各トランスフォーマーレイヤーの自己注意層に学習可能なプレフィックスベクトルを追加
2. プレフィックスベクトルは直接最適化せず、より小さなMLP を通じて生成（パラメータ効率のため）
3. 元のモデルパラメータは凍結し、プレフィックスパラメータのみを訓練

##### 実装例

以下は Prefix Tuning を Hugging Face の PEFT ライブラリを使って実装する例です。

```python title="Prefix Tuning の実装例"
from peft import PrefixTuningConfig, get_peft_model, TaskType

# プレフィックスチューニング設定
prefix_config = PrefixTuningConfig(
    task_type=TaskType.CAUSAL_LM,       # タスクの種類：因果言語モデル
    num_virtual_tokens=20,              # 仮想トークン数：追加するプレフィックストークンの数
    prefix_projection=True,             # プレフィックス投影を使用するか：MLP を通じた投影の有無
    encoder_hidden_size=512,            # プレフィックス投影の隠れ層サイズ：MLP の中間層の次元
)

# ベースモデルにプレフィックスチューニングを適用
peft_model = get_peft_model(base_model, prefix_config)

# 訓練可能なパラメータの確認
trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in peft_model.parameters())
print(f"訓練可能パラメータ: {trainable_params} ({100 * trainable_params / total_params:.2f}%)")
```

このコード例の解説：

1. `task_type=TaskType.CAUSAL_LM` - タスクの種類を指定します。生成モデル（GPT など）の場合は CAUSAL_LM、双方向モデル（BERT など）の場合は SEQ_CLS や SEQ_2_SEQ_LM を使用します。
2. `num_virtual_tokens=20` - プレフィックスとして追加する仮想トークンの数を指定します。タスクの複雑さに応じて 10〜100 の範囲で調整します。
3. `prefix_projection=True` - 直接プレフィックスパラメータを最適化するのではなく、より小さな MLP を通じて生成するかどうかを指定します。True の場合、パラメータ効率が向上します。
4. `encoder_hidden_size=512` - プレフィックス生成 MLP の隠れ層サイズを指定します。モデルの隠れ層サイズより小さい値を設定するのが一般的です。

訓練可能なパラメータの数を確認すると、通常は全体の 0.1〜1% 程度になります。これはモデルサイズに対して非常に効率的です。

##### 適用事例

Prefix Tuning は以下のようなユースケースで特に効果的です。

- **テキスト生成タスク**: 特に条件付き生成や制御可能な生成で優れた性能を発揮します。例えば、GPT モデルにプレフィックスチューニングを適用して、特定のスタイルや内容の文章を生成するシステムが実装されています。
- **低リソース言語や特定ドメイン**: 限られたデータで効率的に学習できるため、リソースの少ない言語や特定の専門分野への適応に適しています。例えば、少数の医療テキストサンプルだけで医療特化型の文章生成モデルを作成する研究があります。
- **感情や属性の制御**: プレフィックスを通じてテキストの特定の属性（感情、フォーマル度など）を制御できます。例えば、メールやメッセージの自動生成システムで、フォーマル、カジュアル、励ましなど様々なトーンの文章を生成する機能の実装に使用されています。

##### 利点と制約

**利点:**
- 非常にパラメータ効率が高い
- 生成タスクでの性能が良好
- 複数のタスク用プレフィックスを切り替え可能

**制約:**
- 訓練が不安定になりやすい
- プレフィックス長の選択が重要
- 一部のタスクでは他の PEFT 手法より性能が劣る場合がある

#### P-Tuning

P-Tuning は Prefix Tuning の変種で、より単純な実装を提供します。

##### 仕組み

P-Tuning は、入力トークンの埋め込み前に少数の連続的な「プロンプトトークン」を挿入し、これらのプロンプトトークンのみを最適化します。

<Mermaid chart={`
graph TD
    A[入力文] --> B[トークナイザー]
    B --> C[トークン列]
    D[プロンプトエンコーダー] --> E[プロンプトトークン]
    C --> F[トークン列に挿入]
    E --> F
    F --> G[モデル処理]
    G --> H[出力]

    style D fill:#FFD700,stroke:#B8860B,color:#000
    style E fill:#FF6347,stroke:#8B0000,color:#000
`} />

**図の説明**:
この図は P-Tuning の仕組みを示しています。通常のトークン化処理の後、少数の連続的な「プロンプトトークン」を生成し、入力トークン列の適切な位置に挿入します。
これらのプロンプトトークンは小型のエンコーダー（通常は LSTM や MLP）によって生成され、訓練中にはこのエンコーダーのパラメータのみが更新されます。
この方法は入力の最も初期の段階で介入するため実装が比較的簡単で、特にテキスト分類などのタスクで効果的です。
Prefix Tuning と異なり、モデルの内部状態ではなく入力に直接作用する点が特徴です。

1. 小型のプロンプトエンコーダー（通常は LSTM や小さな MLP）を使用
2. 連続的なプロンプトトークンを生成し、入力シーケンスに挿入
3. 元のモデルを凍結し、プロンプトエンコーダーのみを訓練

##### 実装例

以下は P-Tuning を Hugging Face の PEFT ライブラリを使って実装する例です。

```python title="P-Tuning の実装例"
from peft import PromptTuningConfig, get_peft_model, TaskType

# P-Tuning 設定
ptuning_config = PromptTuningConfig(
    task_type=TaskType.CAUSAL_LM,         # タスクの種類：因果言語モデル
    prompt_tuning_init="RANDOM",          # プロンプト初期化方法：ランダムな初期化
    num_virtual_tokens=10,                # 仮想トークン数：追加するプロンプトトークンの数
    tokenizer_name_or_path="gpt2"         # 使用するトークナイザー：GPT-2のトークナイザー
)

# ベースモデルに P-Tuning を適用
peft_model = get_peft_model(base_model, ptuning_config)

# 訓練可能なパラメータの確認
trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in peft_model.parameters())
print(f"訓練可能パラメータ: {trainable_params} ({100 * trainable_params / total_params:.2f}%)")
```

このコード例の解説：

1. `task_type=TaskType.CAUSAL_LM` - タスクの種類を指定します。P-Tuning は特に生成モデルとの相性が良いですが、分類タスクにも使用できます。
2. `prompt_tuning_init="RANDOM"` - プロンプトトークンの初期化方法を指定します。"RANDOM"（ランダム初期化）のほか、"TEXT"（特定のテキストで初期化）も選択できます。
3. `num_virtual_tokens=10` - 追加する仮想プロンプトトークンの数を指定します。タスクの複雑さに応じて調整します。典型的には 5〜20 の範囲です。
4. `tokenizer_name_or_path="gpt2"` - プロンプトトークンのエンコードに使用するトークナイザーを指定します。ベースモデルのトークナイザーと一致させるのが一般的です。

P-Tuning は非常にパラメータ効率が高く、通常は全パラメータの 0.01% 未満しか更新しません。
この特性により、限られたリソースでも大規模モデルを効率的にカスタマイズできます。

##### 適用事例

P-Tuning は以下のようなユースケースで特に効果的です。

- **質問応答システム**: 特に事実型の質問に対して、少ない計算リソースで効率的にモデルをカスタマイズできます。例えば、特定のドメイン（医療、法律など）に特化した QA システムの構築に使用されています。
- **分類タスク**: テキスト分類タスク（感情分析、トピック分類など）において、少ないデータでも良好な性能を発揮します。例えば、カスタマーレビューの感情分析や、ニュース記事のカテゴリ分類などに応用されています。
- **少数ショット学習**: ごく少数の例からタスクを学習する能力が高く、データが限られている状況に適しています。例えば、新しい製品カテゴリの分類や、新しい言語への素早い適応などに使用されています。

##### 利点と制約

**利点:**
- 非常にパラメータ効率が高い
- 実装が比較的単純
- 少ないデータでも効果的な場合が多い

**制約:**
- タスクによっては性能が制限される
- プロンプト長の調整が必要
- 複雑なタスクでは他の PEFT 手法より性能が劣る場合がある

### PEFT 手法の比較

各 PEFT 手法の基本的な特性を以下の表で比較します。

| 手法 | パラメータ効率 | メモリ効率 | 実装の複雑さ | 訓練の安定性 | 推論速度 | 特に適したタスク |
|------|----------------|------------|--------------|--------------|----------|------------------|
| LoRA | ★★★★★ | ★★★★☆ | ★★★★☆ | ★★★★☆ | ★★★★★ | 一般的なNLPタスク、生成 |
| Adapter | ★★★☆☆ | ★★★☆☆ | ★★★☆☆ | ★★★★★ | ★★★☆☆ | マルチタスク、転移学習 |
| Prefix Tuning | ★★★★☆ | ★★★★★ | ★★☆☆☆ | ★★☆☆☆ | ★★★★☆ | 生成タスク |
| P-Tuning | ★★★★★ | ★★★★★ | ★★★★☆ | ★★★☆☆ | ★★★★★ | 分類タスク、簡単な生成 |

#### PEFT 手法の詳細比較

以下の表は各 PEFT 手法についてより詳細な比較を提供します。

| 手法 | 特徴 | パラメータ効率 | 適用箇所 | 訓練コスト | 長所 | 短所 | 代表実装・ライブラリ例 |
|------|------|----------------|----------|------------|------|------|----------------------|
| **LoRA** | 重み行列を2つの低ランク行列の積に分解し、差分として適用 | 非常に高い（元の1〜2%程度） | Attention/MLPの重み行列 | 低 | 元のモデルをほぼ固定、パラメータ削減効果が大きい | 行列分解のrank調整がチューニングに影響 | Hugging Face PEFT, `peft` |
| **Adapter** | 各層の間に小さなネットワーク（Adapter block）を追加 | 中程度（数%〜10%程度） | Transformer層の間 | 中 | モジュール単位で管理が可能、複数タスクの切替に強い | モデルに構造変更が入るため実装複雑 | AdapterHub, Hugging Face PEFT |
| **Prefix Tuning** | 各層にタスク特化の「prefix（仮想トークン）」を埋め込む | 高い（1〜2%以下） | Attentionのキー/バリュー入力 | 低 | 元のパラメータは固定、複数タスク学習との相性が良い | 文脈制御に限定される、長文依存の精度が低下する場合あり | Hugging Face PEFT |
| **P-Tuning (v2)** | Embeddingレイヤに学習可能な仮想トークンを挿入 | 高い（2〜5%程度） | 入力トークンエンコーディング部 | 低〜中 | 非常に軽量、プロンプト設計の自動化に近いアプローチ | 長文に弱い可能性、入力の前処理に工夫が必要 | OpenPrompt, Hugging Face PEFT |

### PEFT の実装フレームワーク

PEFT 手法を簡単に実装するためのフレームワークとライブラリがいくつか存在します。

#### Hugging Face PEFT ライブラリ

Hugging Face の PEFT ライブラリは、様々な PEFT 手法を統一的なインターフェースで提供しています。

```python title="Hugging Face PEFT ライブラリの使用例"
# インストール
# pip install peft

from transformers import AutoModelForCausalLM
from peft import get_peft_model, LoraConfig, TaskType

# ベースモデルのロード
model = AutoModelForCausalLM.from_pretrained("gpt2")

# PEFT 設定
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,          # タスク種類：因果言語モデル
    r=8,                                   # ランク：低ランク行列の次元数
    lora_alpha=32,                         # スケーリング係数：更新の強さを調整
    lora_dropout=0.1,                      # ドロップアウト率：過学習防止
    target_modules=["c_attn", "c_proj"]    # 対象モジュール：LoRAを適用する層
)

# PEFT モデルの取得
peft_model = get_peft_model(model, peft_config)

# 訓練可能なパラメータの表示
trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in peft_model.parameters())
print(f"訓練可能パラメータ: {trainable_params} ({trainable_params/total_params*100:.4f}%)")
```

このコード例の解説：

1. まず `AutoModelForCausalLM` を使って GPT-2 などのベースモデルをロードします。
2. LoRA の設定 `LoraConfig` を作成します：
   - `task_type` でモデルの種類とタスクタイプを指定
   - `r` でランク（次元削減の度合い）を設定
   - `lora_alpha` でスケーリング係数を設定（通常は r の 2〜4 倍）
   - `lora_dropout` で過学習防止のためのドロップアウト率を設定
   - `target_modules` で LoRA を適用する特定の層を指定（モデルアーキテクチャに依存）
3. `get_peft_model` 関数でベースモデルに PEFT 設定を適用します。
4. 最後に、訓練可能なパラメータの数とその割合を確認します。典型的には全体の 1% 未満になります。

#### 実際のアプリケーション例

PEFT ライブラリは以下のような実際のアプリケーションで活用されています。

- **特定ドメインへの適応**: 法律文書生成、医療レポート作成、科学論文要約など特定領域向けモデルの効率的な構築
- **多言語対応**: 限られたリソースでの低リソース言語への適応（翻訳、要約、Q&A など）
- **パーソナライゼーション**: ユーザーごとに異なるアダプターを用意し、個人化された応答を生成するシステム
- **教育アプリケーション**: 特定の教育内容に適応したチューターAIの開発

#### Adapter-Transformers

Adapter-Transformers は、特にアダプターに焦点を当てたライブラリで、様々なアダプター実装を提供しています。

```python title="Adapter-Transformers ライブラリの使用例"
# インストール
# pip install adapter-transformers

from transformers import AutoModelWithHeads

# アダプターを使用可能なモデルのロード
model = AutoModelWithHeads.from_pretrained("bert-base-uncased")

# アダプターの追加
model.add_adapter("my-adapter", config="pfeiffer")

# アダプターを訓練モードに設定
model.train_adapter("my-adapter")

# 訓練可能なパラメータの確認
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())
print(f"訓練可能パラメータ: {trainable_params} ({trainable_params/total_params*100:.4f}%)")
```

### 実践的な PEFT 適用ガイドライン

PEFT を実際に応用する際の指針を以下に示します。

#### PEFT 手法の選択基準

適切な PEFT 手法を選択するためのガイドラインは以下の通りです。

1. **リソース制約**
   - 非常に限られたリソース → LoRA または P-Tuning
   - 中程度のリソース → Adapter または Prefix Tuning

2. **タスクの種類**
   - テキスト生成 → LoRA または Prefix Tuning
   - 分類・理解タスク → Adapter または LoRA
   - マルチタスク → Adapter（モジュール性が高い）

3. **データ量**
   - 少量のデータ → P-Tuning または Prefix Tuning
   - 大量のデータ → LoRA または Adapter

#### ハイパーパラメータ最適化のヒント

PEFT 手法のハイパーパラメータ最適化について、手法別の推奨事項は以下の通りです。

##### LoRA

- **ランク r**: 通常 4〜32 の範囲。データ量が多い場合は大きい値、少ない場合は小さい値
- **スケーリング係数 α**: 通常は r の 2 倍程度に設定
- **ターゲットモジュール**: 注意機構の重み（通常は `query` と `value` の投影層）を優先的に選択

##### Adapter

- **ボトルネックサイズ**: モデルサイズと計算リソースによって 16〜128 の範囲
- **初期化**: 「近距離初期化」（near-identity initialization）が推奨

##### Prefix Tuning

- **プレフィックス長**: タスクの複雑さに応じて 10〜100 の範囲
- **レイテント次元**: 元の隠れ層の 1/4〜1/2 程度

#### 学習率と最適化設定

PEFT モデルの訓練には以下の設定が効果的です。

- 学習率: 通常のファインチューニングより低い値（1e-4〜1e-5）が適切
- 最適化アルゴリズム: AdamW が一般的に良好な結果を示す
- 学習率スケジューリング: 線形減衰スケジュールが効果的
- エポック数: 少ないエポック（3〜10）で十分な場合が多い

```python title="PEFT モデルの訓練実装例"
from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    learning_rate=5e-5,
    per_device_train_batch_size=8,
    gradient_accumulation_steps=4,
    num_train_epochs=5,
    warmup_steps=100,
    weight_decay=0.01,
    lr_scheduler_type="linear",
    save_strategy="epoch",
    evaluation_strategy="epoch",
)

trainer = Trainer(
    model=peft_model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=data_collator,
)

trainer.train()
```

### 産業別の PEFT 実装事例

さまざまな産業分野で PEFT 手法が実際に活用されている例を紹介します。

#### 医療分野での PEFT 応用

医療分野では、専門知識への適応と計算リソースの効率化の両面から PEFT が活用されています。

- **症例報告生成システム**: Mayo Clinic では LoRA を使用して放射線科レポートの自動生成システムを開発。10,000例の専門的な放射線科レポートでファインチューニングすることで、医学用語の正確な使用と適切な所見の記述が可能になりました。全パラメータファインチューニングと比較して、5倍の速度向上と1/10のメモリ使用量削減を達成しています。

- **医療文献 QA システム**: 製薬企業では、Adapter 手法を利用して複数の疾患領域に特化した QA システムを構築。ベースモデルに複数のアダプターを追加し、疾患ごとに切り替えて使用することで、特定疾患の専門的な質問により正確に回答できるようになりました。各疾患アダプターのサイズは数 MB に抑えられており、モバイルデバイスでも使用可能な軽量性を実現しています。

#### 金融分野での PEFT 応用

金融機関では規制対応と個別化された顧客サービスのために PEFT が活用されています。

- **規制文書の解析と要約**: 大手金融機関では Prefix Tuning を活用して、複雑な金融規制文書の解析と要約システムを構築。規制文書特有の専門用語や形式に適応し、重要なコンプライアンス要件を抽出・要約する能力を向上させました。従来のファインチューニング手法と比較して、70%のコンピュータリソース削減と規制更新への迅速な対応を実現しています。

- **パーソナライズされた金融アドバイス生成**: FinTech 企業では、LoRA と P-Tuning を組み合わせたハイブリッド手法を採用し、顧客ごとにパーソナライズされた金融アドバイスを生成するシステムを開発。顧客セグメント別に異なる LoRA を適用することで、個々の顧客プロファイルや投資目標に合わせたアドバイスを生成できるようになりました。

#### 製造業での PEFT 応用

製造業では多言語対応と専門知識の活用のために PEFT が利用されています。

- **多言語マニュアル生成**: 多国籍製造企業では、Adapter Tuning を活用して技術マニュアルの多言語生成システムを構築。ベースモデルに言語ごとのアダプターを追加することで、20以上の言語に対応し、技術用語の正確な翻訳と一貫性のある説明を実現しています。新言語への対応も、新たなアダプターを追加するだけで済むため、拡張性が高く維持コストも削減されています。

- **産業機器の故障診断支援**: 重工業メーカーでは、LoRA を用いて産業機器の故障診断を支援するAIシステムを開発。10万件以上の過去の故障事例と修理記録でファインチューニングし、症状の説明から考えられる原因と解決策を提案できるようになりました。30GB以上の専門マニュアルと故障事例を学習させながらも、訓練に必要なGPUメモリを75%削減することに成功しています。

#### 小売・Eコマース分野での PEFT 応用

小売業では顧客体験の向上と業務効率化のために PEFT が活用されています。

- **商品説明の自動生成**: オンラインマーケットプレイスでは、P-Tuning を活用して商品カテゴリ別に最適化された商品説明文を自動生成するシステムを開発。カテゴリごとに異なるスタイルや強調すべき特徴を学習させ、魅力的で情報量の多い商品説明を生成しています。各カテゴリの特性に応じたプロンプトトークンの最適化により、コンバージョン率が平均15%向上しました。

- **カスタマーサポート自動化**: 大手小売チェーンでは、Adapter と LoRA を組み合わせたアプローチで、多言語・多チャネル対応のカスタマーサポートシステムを構築。顧客の問い合わせ内容に応じて適切なアダプターを選択し、一貫性のある回答を提供しています。PEFT 手法の採用により、全言語・全チャネルで共通のベースモデルを使用しながらも、特定シナリオでの応答精度を向上させることに成功しました。

### 高度な PEFT テクニック

より高度な PEFT テクニックについて以下に紹介します。

#### マルチタスク PEFT

複数のタスクに対して単一のベースモデルを使用する手法です。

```python title="マルチタスク PEFT の実装例"
# 複数のアダプターを追加
model.add_adapter("task1", config="pfeiffer")
model.add_adapter("task2", config="pfeiffer")

# タスク1の訓練
model.train_adapter("task1")
trainer1.train()

# タスク2の訓練
model.train_adapter("task2")
trainer2.train()

# 推論時にスイッチ
model.set_active_adapters("task1")  # タスク1用の予測
model.set_active_adapters("task2")  # タスク2用の予測
```

**マルチタスク PEFT の利点**:
このアプローチでは、基盤となる大規模モデルを共有しながら、タスク固有の小さなアダプターを切り替えて使用します。
これにより、複数のタスクを効率的に処理できるだけでなく、タスク間での知識共有も可能になります。
企業環境では、同じベースモデルを使ってさまざまな部門や用途向けのカスタマイズが実現できるため、インフラコストを大幅に削減できます。

#### PEFT 手法の組み合わせ

複数の PEFT 手法を組み合わせて使用することでさらなる性能向上が期待できます。

```python tile="複数 PEFT 手法の実装例"
from peft import LoraConfig, PrefixTuningConfig, MultitaskPromptTuningConfig

# LoRA + Prefix Tuning の組み合わせ例
lora_config = LoraConfig(...)
prefix_config = PrefixTuningConfig(...)

# 最初に LoRA を適用
peft_model = get_peft_model(base_model, lora_config)
# 次に Prefix Tuning を適用
peft_model = get_peft_model(peft_model, prefix_config)
```

**組み合わせ手法の実装事例**:
Amazon のリサーチチームは、商品レビュー分析システムにおいて LoRA と Prefix Tuning を組み合わせたハイブリッドアプローチを採用しました。
LoRA で言語理解の基本性能を向上させつつ、Prefix Tuning で商品カテゴリごとの特殊性を捉えることに成功。
この組み合わせにより、単一手法と比較して感情分析の精度が 3.5% 向上し、特に曖昧な表現の解釈において顕著な改善が見られました。

#### PEFT モデルの量子化と最適化

PEFT モデルをさらに軽量化するための技術です。

```python title="PEFT モデルの量子化例"
from transformers import BitsAndBytesConfig
import bitsandbytes as bnb

# 4ビット量子化設定
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,
    bnb_4bit_quant_type="nf4"
)

# 量子化されたベースモデルのロード
base_model = AutoModelForCausalLM.from_pretrained(
    "meta-llama/Llama-2-7b-hf",
    quantization_config=quantization_config,
    device_map="auto"
)

# PEFT の適用
peft_model = get_peft_model(base_model, peft_config)
```

**量子化と PEFT の組み合わせ効果**:
ニューヨーク大学の研究グループは、65B パラメータの大規模言語モデルに 4ビット量子化と LoRA を組み合わせることで、16GB の消費者向け GPU でも訓練・推論が可能なシステムを開発しました。
従来は数百 GB のメモリを必要としていたモデルが、精度をほぼ維持したまま（性能低下わずか 1.2%）大幅に軽量化され、エッジデバイスでの実行も視野に入れた実装が可能になりました。

### 今後の展望と課題

PEFT 手法の現状の課題と将来の方向性について以下に示します。

#### 課題

- 一部のタスクでは全パラメータファインチューニングほどの性能に達しない
- 非常に少量のデータでの効果が限定的な場合がある
- ハイパーパラメータの選択に敏感
- 特定のモデルアーキテクチャに対する特化した調整が必要な場合がある
- タスクやデータセットによって手法ごとの性能にばらつきがある
- ハイパーパラメータ選定における最適解の難しさ
- モデル圧縮や蒸留との併用時のベストプラクティスが未確立

#### 将来の方向性

- 異なる PEFT 手法の最適な組み合わせ
- 自動ハイパーパラメータ選択手法
- マルチモーダルモデルへの PEFT 適用
- 新しいパラメータ効率の高い手法の開発
- より少ないデータでの効果的な PEFT 手法
- 複数の PEFT 手法のハイブリッド化（例：LoRA + Prefix）
- 動的 PEFT（タスクやユーザに応じて適応手法を切り替える）
- より高速な訓練・デプロイのためのライブラリサポート拡充（例：PEFT の進化）

### まとめ

PEFT 手法は、限られた計算リソースでの効率的なモデルカスタマイズを可能にし、AI の民主化に貢献しています。
LoRA、Adapter、Prefix Tuning などの手法は、それぞれ独自の特性を持ち、異なるユースケースに適しています。
適切な PEFT 手法とハイパーパラメータの選択により、大規模言語モデルを効率的にカスタマイズし、特定のタスクやドメインに適応させることが可能です。

### 用語解説

| 用語 | 説明 |
|------|------|
| PEFT | Parameter-Efficient Fine-Tuning の略。モデルの一部のパラメータのみを更新する効率的なファインチューニング手法の総称 |
| LoRA | Low-Rank Adaptation の略。重み行列の更新を低ランク行列の積で近似する手法 |
| Adapter | モデルのアーキテクチャに小さなニューラルネットワークを挿入する手法 |
| Prefix Tuning | モデルの入力に学習可能なプレフィックスベクトルを追加する手法 |
| P-Tuning | Prefix Tuning の変種で、より単純な実装を提供する手法 |
| ハイパーパラメータ | モデルの学習過程を制御するパラメータ。学習率、バッチサイズなど |
| ランク (r) | LoRA における低ランク行列の次元数 |
| ボトルネック | Adapter における中間層の縮小された次元数 |
| 量子化 | モデルパラメータの精度を下げてメモリ使用量を削減する技術 |
